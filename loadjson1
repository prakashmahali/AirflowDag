import json
from google.cloud import storage
from airflow.providers.google.cloud.hooks.gcs import GCSHook

def preprocess_json(bucket_name, source_blob_name, destination_blob_name):
    client = storage.Client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(source_blob_name)
    json_data = blob.download_as_text()
    records = json.loads(json_data)
    
    def sanitize_key(key):
        # Replace or remove special characters as needed
        return key.replace(' ', '_').replace('-', '_').replace('.', '_')
    
    sanitized_records = [
        {sanitize_key(key): value for key, value in record.items()}
        for record in records
    ]
    
    sanitized_blob = bucket.blob(destination_blob_name)
    sanitized_blob.upload_from_string(json.dumps(sanitized_records), content_type='application/json')

def preprocess_all_json_files(bucket_name, prefix):
    client = storage.Client()
    blobs = client.list_blobs(bucket_name, prefix=prefix)
    sanitized_files = []
    
    for blob in blobs:
        if blob.name.endswith('.json'):
            source_blob_name = blob.name
            destination_blob_name = f'sanitized_{source_blob_name}'
            preprocess_json(bucket_name, source_blob_name, destination_blob_name)
            sanitized_files.append(destination_blob_name)
    
    return sanitized_files



from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.operators.python import PythonOperator
from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
}

dag = DAG(
    'gcs_to_bigquery_sanitized',
    default_args=default_args,
    description='Preprocess JSON files and load into BigQuery',
    schedule_interval=None,
    start_date=days_ago(1),
    tags=['example'],
)

bucket_name = 'your-gcs-bucket-name'
prefix = 'path/to/json/files/'
project_id = 'your-gcp-project-id'
dataset_id = 'your_bq_dataset_id'
table_id = 'your_bq_table_id'

# Task to preprocess all JSON files
preprocess_task = PythonOperator(
    task_id='preprocess_all_json_files',
    python_callable=preprocess_all_json_files,
    op_kwargs={
        'bucket_name': bucket_name,
        'prefix': prefix,
    },
    dag=dag,
)

# Function to load sanitized files into BigQuery
def load_sanitized_files_to_bq(bucket_name, sanitized_files, project_id, dataset_id, table_id):
    for file in sanitized_files:
        load_task = GCSToBigQueryOperator(
            task_id=f'load_{file.replace("/", "_")}_to_bq',
            bucket=bucket_name,
            source_objects=[file],
            destination_project_dataset_table=f'{project_id}.{dataset_id}.{table_id}',
            source_format='NEWLINE_DELIMITED_JSON',
            write_disposition='WRITE_APPEND',
            create_disposition='CREATE_IF_NEEDED',
            schema_fields=[
                {'name': 'column1', 'type': 'STRING', 'mode': 'NULLABLE'},
                # Add other schema fields as needed
            ],
            dag=dag,
        )
        load_task.execute(dict())

# Task to load data into BigQuery
load_to_bq_task = PythonOperator(
    task_id='load_sanitized_files_to_bq',
    python_callable=load_sanitized_files_to_bq,
    op_kwargs={
        'bucket_name': bucket_name,
        'sanitized_files': '{{ task_instance.xcom_pull(task_ids="preprocess_all_json_files") }}',
        'project_id': project_id,
        'dataset_id': dataset_id,
        'table_id': table_id,
    },
    dag=dag,
)

preprocess_task >> load_to_bq_task
